#!/usr/bin/env python3
"""
Hajimi King - ä¼˜åŒ–ç‰ˆä¸»ç¨‹åº
ä½¿ç”¨å¹¶å‘éªŒè¯ã€æ™ºèƒ½Tokenç®¡ç†ã€å®‰å…¨æ—¥å¿—ç­‰ä¼˜åŒ–åŠŸèƒ½
"""

import os
import sys
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
import signal

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from common.config import Config
from utils.file_manager import FileManager, Checkpoint
from utils.github_utils_enhanced import EnhancedGitHubUtils as GitHubUtils
from utils.concurrent_validator import concurrent_validator
from utils.secure_logger import secure_logger
from utils.token_manager import SmartTokenManager
from utils.stats_reporter import stats_reporter
from utils.session_manager import GitHubSessionManager


class HajimiKingOptimized:
    """ä¼˜åŒ–ç‰ˆçš„Hajimi Kingä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        secure_logger.info("=" * 60)
        secure_logger.info("ğŸš€ HAJIMI KING OPTIMIZED STARTING")
        secure_logger.info("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.file_manager = FileManager(Config.DATA_PATH)
        self.token_manager = SmartTokenManager(Config.GITHUB_TOKENS)
        self.github_session = GitHubSessionManager(Config.GITHUB_TOKENS)
        self.github_utils = GitHubUtils.create_instance(Config.GITHUB_TOKENS)
        
        # ç»Ÿè®¡æŠ¥å‘Šå™¨
        stats_reporter.start_scan()
        
        # ä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.running = True
        secure_logger.info("âœ… System initialized successfully")
    
    def _signal_handler(self, signum, frame):
        """å¤„ç†ç»ˆæ­¢ä¿¡å·"""
        secure_logger.info("\nâ›” Received termination signal, shutting down gracefully...")
        self.running = False
        self._cleanup()
        sys.exit(0)
    
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        secure_logger.info("ğŸ§¹ Cleaning up resources...")
        
        # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
        stats_reporter.end_scan()
        stats_reporter.print_summary()
        stats_reporter.generate_html_report()
        
        # å…³é—­éªŒè¯å™¨
        concurrent_validator.shutdown()
        
        # æ˜¾ç¤ºTokençŠ¶æ€
        token_summary = self.token_manager.get_status_summary()
        secure_logger.info(f"ğŸ“Š Token usage summary: {token_summary['total_requests']} requests, "
                          f"{token_summary['success_rate']:.1%} success rate")
        
        secure_logger.info("âœ… Cleanup completed")
    
    def check_system(self) -> bool:
        """ç³»ç»Ÿæ£€æŸ¥"""
        secure_logger.info("ğŸ” Running system checks...")
        
        if not Config.check():
            secure_logger.error("âŒ Configuration check failed")
            return False
        
        if not self.file_manager.check():
            secure_logger.error("âŒ File manager check failed")
            return False
        
        secure_logger.info("âœ… All system checks passed")
        return True
    
    def extract_keys_from_content(self, content: str) -> Dict[str, List[str]]:
        """ä»å†…å®¹ä¸­æå–å¯†é’¥"""
        import re
        
        keys = {"gemini": [], "openrouter": []}
        api_key_type = Config.API_KEY_TYPE.lower()
        
        if api_key_type in ["gemini", "both"]:
            gemini_pattern = r'(AIzaSy[A-Za-z0-9\-_]{33})'
            keys["gemini"] = re.findall(gemini_pattern, content)
        
        if api_key_type in ["openrouter", "both"]:
            openrouter_pattern = r'(sk-or-[A-Za-z0-9\-_]{20,50})'
            keys["openrouter"] = re.findall(openrouter_pattern, content)
        
        return keys
    
    def filter_placeholder_keys(self, keys_by_type: Dict[str, List[str]], content: str) -> Dict[str, List[str]]:
        """è¿‡æ»¤å ä½ç¬¦å¯†é’¥"""
        filtered = {}
        
        for key_type, key_list in keys_by_type.items():
            filtered_keys = []
            for key in key_list:
                context_index = content.find(key)
                if context_index != -1:
                    snippet = content[context_index:context_index + 45]
                    if "..." in snippet or "YOUR_" in snippet.upper() or "PLACEHOLDER" in snippet.upper():
                        continue
                filtered_keys.append(key)
            filtered[key_type] = list(set(filtered_keys))  # å»é‡
        
        return filtered
    
    def process_item(self, item: Dict[str, Any]) -> Tuple[int, int]:
        """
        å¤„ç†å•ä¸ªæœç´¢ç»“æœé¡¹
        
        Returns:
            (valid_count, rate_limited_count)
        """
        repo_name = item["repository"]["full_name"]
        file_path = item["path"]
        file_url = item["html_url"]
        
        # æ›´æ–°ç»Ÿè®¡
        stats_reporter.add_scanned_item()
        
        # è·å–æ–‡ä»¶å†…å®¹
        content = self.github_utils.get_file_content(item)
        if not content:
            secure_logger.warning(f"âš ï¸ Failed to fetch: {file_url}")
            stats_reporter.add_error("fetch_failed", f"Could not fetch {file_url}")
            return 0, 0
        
        # æå–å¯†é’¥
        keys_by_type = self.extract_keys_from_content(content)
        filtered_keys = self.filter_placeholder_keys(keys_by_type, content)
        
        total_keys = sum(len(keys) for keys in filtered_keys.values())
        if total_keys == 0:
            return 0, 0
        
        secure_logger.info(f"ğŸ”‘ Found {total_keys} potential keys in {repo_name}/{file_path}")
        
        # å¹¶å‘éªŒè¯å¯†é’¥
        validation_results = concurrent_validator.validate_batch(filtered_keys)
        
        valid_keys = []
        rate_limited_keys = []
        
        for key_type, results in validation_results.items():
            for result in results:
                # æ›´æ–°ç»Ÿè®¡
                stats_reporter.add_found_key(key_type, f"{repo_name}/{file_path}", result.is_valid)
                
                if result.is_valid:
                    valid_keys.append(result.key)
                    secure_logger.log_key_validation(key_type, result.key, True, result.details)
                elif result.rate_limited:
                    rate_limited_keys.append(result.key)
                    stats_reporter.add_rate_limited_key(key_type)
                    secure_logger.log_key_validation(key_type, result.key, False, {"reason": "rate_limited"})
                else:
                    secure_logger.log_key_validation(key_type, result.key, False, {"error": result.error})
        
        # ä¿å­˜ç»“æœ
        if valid_keys:
            self.file_manager.save_valid_keys(repo_name, file_path, file_url, valid_keys)
            secure_logger.info(f"ğŸ’¾ Saved {len(valid_keys)} valid keys")
        
        if rate_limited_keys:
            self.file_manager.save_rate_limited_keys(repo_name, file_path, file_url, rate_limited_keys)
            secure_logger.info(f"ğŸ’¾ Saved {len(rate_limited_keys)} rate limited keys")
        
        return len(valid_keys), len(rate_limited_keys)
    
    def should_skip_item(self, item: Dict[str, Any], checkpoint: Checkpoint, force_full_scan: bool) -> Tuple[bool, str]:
        """æ£€æŸ¥æ˜¯å¦åº”è·³è¿‡æ­¤é¡¹"""
        # æ£€æŸ¥SHA
        if not force_full_scan and item.get("sha") in checkpoint.scanned_shas:
            stats_reporter.add_skipped_item("sha_duplicate")
            return True, "sha_duplicate"
        
        # æ£€æŸ¥ä»“åº“å¹´é¾„
        repo_pushed_at = item["repository"].get("pushed_at")
        if repo_pushed_at:
            repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
            if repo_pushed_dt < datetime.utcnow() - timedelta(days=Config.DATE_RANGE_DAYS):
                stats_reporter.add_skipped_item("age_filter")
                return True, "age_filter"
        
        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„é»‘åå•
        lowercase_path = item["path"].lower()
        if any(token in lowercase_path for token in Config.FILE_PATH_BLACKLIST):
            stats_reporter.add_skipped_item("doc_filter")
            return True, "doc_filter"
        
        return False, ""
    
    def process_query(self, query: str, checkpoint: Checkpoint, force_full_scan: bool) -> Tuple[int, int]:
        """
        å¤„ç†å•ä¸ªæŸ¥è¯¢
        
        Returns:
            (valid_count, rate_limited_count)
        """
        secure_logger.info(f"ğŸ” Processing query: {query}")
        
        # ä½¿ç”¨sessionç®¡ç†å™¨è¿›è¡Œæœç´¢
        result = self.github_utils.search_for_keys(query)
        
        if not result or "items" not in result:
            secure_logger.warning(f"âŒ Query failed: {query}")
            stats_reporter.add_error("query_failed", f"Query failed: {query}")
            return 0, 0
        
        items = result["items"]
        if not items:
            secure_logger.info(f"ğŸ“­ No items found for query: {query}")
            return 0, 0
        
        secure_logger.info(f"ğŸ“¦ Found {len(items)} items for query: {query}")
        
        valid_count = 0
        rate_limited_count = 0
        
        for i, item in enumerate(items, 1):
            if not self.running:
                break
            
            # æ£€æŸ¥æ˜¯å¦è·³è¿‡
            should_skip, reason = self.should_skip_item(item, checkpoint, force_full_scan)
            if should_skip:
                secure_logger.debug(f"â­ï¸ Skipping item {i}/{len(items)}: {reason}")
                continue
            
            # å¤„ç†é¡¹
            item_valid, item_limited = self.process_item(item)
            valid_count += item_valid
            rate_limited_count += item_limited
            
            # è®°å½•å·²æ‰«æ
            checkpoint.add_scanned_sha(item.get("sha"))
            
            # å®šæœŸä¿å­˜è¿›åº¦
            if i % 10 == 0:
                self.file_manager.save_checkpoint(checkpoint)
                stats_reporter.save_report()
                secure_logger.info(f"ğŸ“ˆ Progress: {i}/{len(items)} items processed")
        
        return valid_count, rate_limited_count
    
    def run(self):
        """ä¸»è¿è¡Œå¾ªç¯"""
        if not self.check_system():
            secure_logger.error("âŒ System check failed, exiting...")
            return
        
        # åŠ è½½æŸ¥è¯¢å’Œæ£€æŸ¥ç‚¹
        search_queries = self.file_manager.get_search_queries()
        checkpoint = self.file_manager.load_checkpoint()
        
        secure_logger.info(f"ğŸ“‹ Loaded {len(search_queries)} queries")
        stats_reporter.update_query_stats(len(search_queries), 0)
        
        # æ£€æŸ¥æ˜¯å¦å…¨é‡æ‰«æ
        force_full_scan = checkpoint.should_force_full_scan()
        if force_full_scan:
            secure_logger.info("ğŸ”„ Full scan mode activated")
            checkpoint.reset_for_full_scan()
        
        loop_count = 0
        total_valid = 0
        total_limited = 0
        
        while self.running:
            loop_count += 1
            secure_logger.info(f"\nğŸ”„ Starting loop #{loop_count}")
            
            processed_queries = 0
            
            for query in search_queries:
                if not self.running:
                    break
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                if query in checkpoint.processed_queries and not force_full_scan:
                    continue
                
                # è·å–æœ€ä½³Token
                token = self.token_manager.get_best_token()
                if not token:
                    secure_logger.error("âŒ No available tokens, waiting...")
                    time.sleep(60)
                    continue
                
                # å¤„ç†æŸ¥è¯¢
                valid, limited = self.process_query(query, checkpoint, force_full_scan)
                total_valid += valid
                total_limited += limited
                
                # æ›´æ–°TokençŠ¶æ€
                self.token_manager.update_token_status(token, success=valid > 0 or limited > 0)
                
                # æ ‡è®°æŸ¥è¯¢å·²å¤„ç†
                checkpoint.add_processed_query(query)
                checkpoint.update_scan_time()
                self.file_manager.save_checkpoint(checkpoint)
                
                processed_queries += 1
                stats_reporter.update_query_stats(len(search_queries), processed_queries)
                
                # å®šæœŸæ˜¾ç¤ºç»Ÿè®¡
                if processed_queries % 5 == 0:
                    stats_reporter.print_summary()
                    token_summary = self.token_manager.get_status_summary()
                    secure_logger.info(f"ğŸ¯ Tokens: {token_summary['available_tokens']}/{token_summary['total_tokens']} available")
            
            if force_full_scan:
                checkpoint.update_full_scan_time()
                self.file_manager.save_checkpoint(checkpoint)
                secure_logger.info("âœ… Full scan completed")
            
            secure_logger.info(f"ğŸ Loop #{loop_count} complete - Valid: {total_valid}, Limited: {total_limited}")
            
            # ç”ŸæˆæŠ¥å‘Š
            stats_reporter.save_report()
            stats_reporter.generate_html_report()
            
            # ä¼‘çœ 
            secure_logger.info("ğŸ’¤ Sleeping for 60 seconds...")
            time.sleep(60)
    
    def main(self):
        """ä¸»å…¥å£"""
        try:
            self.run()
        except KeyboardInterrupt:
            secure_logger.info("\nâ›” Interrupted by user")
        except Exception as e:
            secure_logger.error(f"ğŸ’¥ Fatal error: {e}")
            stats_reporter.add_error("fatal", str(e))
        finally:
            self._cleanup()


if __name__ == "__main__":
    app = HajimiKingOptimized()
    app.main()